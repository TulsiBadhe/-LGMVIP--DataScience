{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1eea0a5",
   "metadata": {},
   "source": [
    "# Neural Network to Read Handwriting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab73237",
   "metadata": {},
   "source": [
    "# Advanced Level Task-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596673ba",
   "metadata": {},
   "source": [
    "## Import required libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a5e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import unique , argmax \n",
    "\n",
    "# TensorFlow already contain MNIST data set which can be loaded using Keras\n",
    "import tensorflow as tf # installing tenserflow\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e25aa",
   "metadata": {},
   "source": [
    "## Load the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16963231",
   "metadata": {},
   "source": [
    "The MNIST dataset is a widely used dataset in machine learning and consists of a set of 60,000 training images and 10,000 test images of handwritten digits from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55cf2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data \n",
    "(x_train,y_train), (x_test,y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b254023",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb4572d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "## checking the shapes of training and testing dataset\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350a6936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "#  displayiing the data of x_train\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ca8ef",
   "metadata": {},
   "source": [
    "#### 1.Reshaping the input data which is work as a input for CNN\n",
    "#### 2.The original shape of x_train is (num_train_samples, 28, 28), where num_train_samples is the number of training samples, and 28 by 28 represents the dimensions of each image\n",
    "#### 3.CNN takes the input data with the shape,(num_samples, height, width, channels)..\n",
    "#### 4.Images are grayscale, so the channel value is set to 1\n",
    "#### 5.After reshaping the datatype of array will change to float32 which is common data type used in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2044db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95fd54c",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166fac0",
   "metadata": {},
   "source": [
    "The original pixel values in the MNIST dataset range from 0 to 255, where 0 represents black and 255 represents white.\n",
    "Dividing the pixel values by 255 normalizes the data so that the values are scaled between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56318ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b1e7da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69775bf",
   "metadata": {},
   "source": [
    "## Building the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac2aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb6ddcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7744)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                77450     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,266\n",
      "Trainable params: 96,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d6152",
   "metadata": {},
   "source": [
    "This summary shows that the model has five layers:\n",
    "\n",
    "Conv2D: This layer performs convolutional operations on the input. It has 32 filters of size 3x3 and applies the\n",
    "ReLU activation function. The output shape is (None, 26, 26, 32), where None represents the batch size.\n",
    "\n",
    "MaxPooling2D: This layer performs max pooling, which reduces the spatial dimensions of the input.\n",
    "It uses a 2x2 pooling window. The output shape is (None, 13, 13, 32), as the pooling operation reduces each spatial\n",
    "dimension by a factor of 2.\n",
    "\n",
    "Conv2D: This layer is similar to the previous Conv2D layer but has 64 filters instead of 32.\n",
    "The output shape is (None, 11, 11, 64).\n",
    "\n",
    "Flatten: This layer flattens the previous output, converting it from a 4D tensor to a 2D tensor.\n",
    "The output shape is (None, 7744), where 7744 is the result of multiplying the spatial dimensions.\n",
    "\n",
    "Dense: This layer is a fully connected layer with 10 units and applies the ReLU activation function.\n",
    "It takes the flattened input and produces an output of shape (None, 10), where 10 represents the number of classes in\n",
    "the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34599a",
   "metadata": {},
   "source": [
    "## plot a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eecf189",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27a93219",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c41da1",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a2db6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 49s - loss: 0.2572 - accuracy: 0.9281 - val_loss: 0.0776 - val_accuracy: 0.9756 - 49s/epoch - 165ms/step\n",
      "Epoch 2/10\n",
      "300/300 - 48s - loss: 0.0745 - accuracy: 0.9776 - val_loss: 0.0571 - val_accuracy: 0.9820 - 48s/epoch - 160ms/step\n",
      "Epoch 3/10\n",
      "300/300 - 53s - loss: 0.0532 - accuracy: 0.9838 - val_loss: 0.0505 - val_accuracy: 0.9846 - 53s/epoch - 176ms/step\n",
      "Epoch 4/10\n",
      "300/300 - 45s - loss: 0.0442 - accuracy: 0.9860 - val_loss: 0.0416 - val_accuracy: 0.9867 - 45s/epoch - 151ms/step\n",
      "Epoch 5/10\n",
      "300/300 - 49s - loss: 0.0354 - accuracy: 0.9893 - val_loss: 0.0374 - val_accuracy: 0.9889 - 49s/epoch - 162ms/step\n",
      "Epoch 6/10\n",
      "300/300 - 51s - loss: 0.0305 - accuracy: 0.9905 - val_loss: 0.0383 - val_accuracy: 0.9879 - 51s/epoch - 171ms/step\n",
      "Epoch 7/10\n",
      "300/300 - 51s - loss: 0.0250 - accuracy: 0.9925 - val_loss: 0.0382 - val_accuracy: 0.9880 - 51s/epoch - 170ms/step\n",
      "Epoch 8/10\n",
      "300/300 - 46s - loss: 0.0213 - accuracy: 0.9935 - val_loss: 0.0398 - val_accuracy: 0.9882 - 46s/epoch - 155ms/step\n",
      "Epoch 9/10\n",
      "300/300 - 50s - loss: 0.0188 - accuracy: 0.9937 - val_loss: 0.0387 - val_accuracy: 0.9880 - 50s/epoch - 167ms/step\n",
      "Epoch 10/10\n",
      "300/300 - 48s - loss: 0.0154 - accuracy: 0.9953 - val_loss: 0.0369 - val_accuracy: 0.9888 - 48s/epoch - 161ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10984e6bd30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb4df9",
   "metadata": {},
   "source": [
    "## Grayscale Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c69509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Displaying a Grayscale Image\n",
    "img = x_train[25]\n",
    "plt.imshow(np.squeeze(img) ,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3943b1",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8ed564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 10ms/step - loss: 0.0369 - accuracy: 0.9888\n",
      "Test accuracy: 0.9887999892234802\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6ec39e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.88%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab84c02b",
   "metadata": {},
   "source": [
    "## Predicting the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56b0b73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 594ms/step\n",
      "Predicted class: 2\n"
     ]
    }
   ],
   "source": [
    "img= img.reshape(1, 28, 28, 1)\n",
    "a= model.predict([img])\n",
    "predicted_class = np.argmax(a)\n",
    "print(\"Predicted class: {}\".format(predicted_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb29c5",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ee175",
   "metadata": {},
   "source": [
    "### Prediction accuracy is about 98.88%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6fbd0",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
